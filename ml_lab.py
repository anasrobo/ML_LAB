# -*- coding: utf-8 -*-
"""ML_LAB.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XDJnSY09Q1D7jNXm4wPs6NIr_oOm-KAs
"""

#Experiment-01
from collections import Counter

def compute_mean(numbers):
    return sum(numbers) / len(numbers)

def compute_median(numbers):
    sorted_numbers = sorted(numbers)
    n = len(sorted_numbers)
    if n % 2 == 0:
        mid = n // 2
        return (sorted_numbers[mid - 1] + sorted_numbers[mid]) / 2
    else:
        return sorted_numbers[n // 2]

def compute_mode(numbers):
    count = Counter(numbers)
    max_count = max(count.values())
    mode = [k for k, v in count.items() if v == max_count]
    return mode

def compute_variance(numbers):
    mean = compute_mean(numbers)
    squared_diffs = [(x - mean)**2 for x in numbers]
    return sum(squared_diffs) / len(numbers)

def compute_standard_deviation(numbers):
    variance = compute_variance(numbers)
    return variance ** 0.5

# Example data
data = [1, 2, 3, 4, 5, 6, 6, 7, 8, 8, 8]
mean = compute_mean(data)
median = compute_median(data)
mode = compute_mode(data)
variance = compute_variance(data)
standard_deviation = compute_standard_deviation(data)

print(f"Mean: {mean}")
print(f"Median: {median}")
print(f"Mode: {mode}")
print(f"Variance: {variance}")
print(f"Standard Deviation: {standard_deviation}")

# Experiment - 04
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Load dataset from scikit-learn
housing = fetch_california_housing()
df = pd.DataFrame(housing.data, columns=housing.feature_names)

# Add target column manually
df['target'] = housing.target

# Preview data
print("First 5 rows of the dataset:")
print(df.head())

# Select feature and target
X = df[['AveRooms']]
y = df['target']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Linear Regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Show coefficients
print(f"\nIntercept:\n{model.intercept_}")
print(f"\nCoefficient:\n{model.coef_}")

# Make predictions
y_pred = model.predict(X_test)

# Show sample predictions
predictions = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})
print("\nSample Predictions:")
print(predictions.head())

# Plot actual vs predicted
plt.scatter(X_test, y_test, color='blue', label='Actual Data')
plt.plot(X_test, y_pred, color='red', label='Regression Line')
plt.xlabel('Average Rooms')
plt.ylabel('House Price ($1000s)')
plt.title('Simple Linear Regression')
plt.legend()
plt.show()

# Metrics
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f"\nMean Squared Error: {mse:.4f}")
print(f"R-Squared Score: {r2:.4f}")

# Experiment - 05
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.datasets import fetch_california_housing
from mpl_toolkits.mplot3d import Axes3D

# Load the dataset
california = fetch_california_housing()

# Select only the features for the 3D plot
X = pd.DataFrame(california.data, columns = california.feature_names)
y = pd.Series(california.target)
# Select only the features for the 3D plot
X_plot_features = X[['MedInc', 'AveRooms']]


X_all = pd.DataFrame(california.data, columns = california.feature_names)

X_all['HousePrice'] = california.target
print("First 5 rows of the dataset :")
print(X_all.head())

# Split the data into training and testing sets using only the selected features
X_train, X_test, y_train, y_test = train_test_split(X_plot_features, y, test_size=0.2, random_state=42)

#Train the model using only the selected features
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions - This is for evaluating the model on the test set,
# using the same features it was trained on.
y_pred = model.predict(X_test)

#Create a 3D plot
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')

#Scatter plot of actual data - Use the selected test features
ax.scatter(X_test['MedInc'], X_test['AveRooms'], y_test, color='blue', label='Actual Data')

#Create a mesh grid for prediction surface
X1_range = np.linspace(X_test['MedInc'].min(), X_test['MedInc'].max(), 100)
X2_range = np.linspace(X_test['AveRooms'].min(), X_test['AveRooms'].max(), 100)
X1,X2 = np.meshgrid(X1_range, X2_range)

#Predict over the mesh grid - The grid now has the correct feature names
grid = pd.DataFrame(np.c_[X1.ravel(), X2.ravel()],columns = ['MedInc', 'AveRooms'])

# Predict using the model trained on only 'MedInc' and 'AveRooms'
Z = model.predict(grid).reshape(X1.shape)

#Plot Regression surface
ax.plot_surface(X1, X2, Z, color='red', alpha=0.5, rstride = 100, cstride = 100, label='Regression Surface')

#Set labels and title
ax.set_xlabel('Median Income')
ax.set_ylabel('Average Rooms')
ax.set_zlabel('House Price')
ax.set_title('Multiple Linear Regression on California Housing Dataset Best Fit Line (3D)')
ax.legend()
plt.show()

# Experiment - 06
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

# Load Iris Dataset
iris = load_iris()
X = iris.data
Y = iris.target

df = pd.DataFrame(X, columns=iris.feature_names)
df['Target'] = [iris.target_names[i] for i in Y]
print("First 5 rows of the dataset:")
print(df.head())

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Decision Tree with GridSearch
clf = DecisionTreeClassifier(criterion='entropy', random_state=42)
param_grid = {
    'max_depth': [3, 4],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2]
}
grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy', n_jobs=1)
grid_search.fit(X_train, y_train)

# Output results
print("Best Parameters:", grid_search.best_params_)
print(f"Best cross-validation accuracy: {grid_search.best_score_:.4f}")

# Evaluate on test set
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)
accuracy = np.sum(y_pred == y_test) / len(y_test)
print(f"Test Accuracy: {accuracy:.4f}")

# Plot the decision tree
fig, ax = plt.subplots(figsize=(12, 12))
tree.plot_tree(best_model, feature_names=iris.feature_names, class_names=iris.target_names, filled=True)
plt.title("Tuned Decision Tree Classifier (ID3)")
plt.show()

# Experiment - 07
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier  # fixed import
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Load Iris Dataset
iris = load_iris()
X = iris.data
Y = iris.target

df = pd.DataFrame(X, columns=iris.feature_names)
df['Target'] = [iris.target_names[i] for i in Y]
print("First 5 rows of the dataset:")
print(df.head())

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Standardize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# KNN Model
knn = KNeighborsClassifier(n_neighbors=5)  # fixed typo: "KNeighboursClassifier" âžœ "KNeighborsClassifier"
knn.fit(X_train_scaled, y_train)

# Predictions
y_pred = knn.predict(X_test_scaled)

# Evaluation
accuracy = accuracy_score(y_test, y_pred)
confusion_mat = confusion_matrix(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred)
print("\nAccuracy:", accuracy)
print("\nConfusion Matrix:\n", confusion_mat)
print("\nClassification Report:\n", classification_rep)

# Visualization (using first two features)
X_vis = iris.data[:, :2]
Y_vis = iris.target
knn_vis = KNeighborsClassifier(n_neighbors=5)
knn_vis.fit(X_vis, Y_vis)

# Create meshgrid
X_min, X_max = X_vis[:, 0].min() - 1, X_vis[:, 0].max() + 1
Y_min, Y_max = X_vis[:, 1].min() - 1, X_vis[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(X_min, X_max, 0.1),
                     np.arange(Y_min, Y_max, 0.1))

z = knn_vis.predict(np.c_[xx.ravel(), yy.ravel()])
z = z.reshape(xx.shape)

# Plot decision boundaries and data
plt.figure(figsize=(8, 6))
plt.contourf(xx, yy, z, alpha=0.8, cmap=plt.cm.Paired)
plt.scatter(X_vis[:, 0], X_vis[:, 1], c=Y_vis, edgecolors='k', marker='o', s=80, linewidth=1, cmap=plt.cm.Paired)

plt.xlabel('Sepal Length')
plt.ylabel('Sepal Width')
plt.title('K-Nearest Neighbors Classifier (k=5)')
plt.grid(True)
plt.show()

# Experiment - 08
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Generate synthetic dataset
X, y = make_classification(
    n_samples=2000, n_features=2, n_classes=2,
    n_informative=2, n_redundant=0, class_sep=2.0, random_state=42
)

# Data before scaling
df_before = pd.DataFrame(X, columns=['Feature 1', 'Feature 2'])
df_before['Target'] = y
print("First 5 rows of the dataset (before scaling):")
print(df_before.head())

# Initial train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Scale the data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Show scaled training data
df_after = pd.DataFrame(X_train_scaled, columns=['Feature 1_scaled', 'Feature 2_scaled'])
df_after['Target'] = y_train
print("\nFirst 5 rows of the scaled training dataset:")
print(df_after.head())

# Logistic Regression model
model = LogisticRegression(C=10.0, solver='lbfgs', max_iter=1000)
model.fit(X_train_scaled, y_train)

# Prediction & evaluation
y_pred = model.predict(X_test_scaled)
accuracy = accuracy_score(y_test, y_pred)
confusion_mat = confusion_matrix(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred)

print("\nAccuracy:", accuracy)
print("\nConfusion Matrix:\n", confusion_mat)
print("\nClassification Report:\n", classification_rep)

# Plotting decision boundary
if X_train_scaled.shape[1] == 2:
    X_all = np.vstack((X_train_scaled, X_test_scaled))
    y_all = np.hstack((y_train, y_test))
    h = 0.02
    X_min, X_max = X_all[:, 0].min() - 0.5, X_all[:, 0].max() + 0.5
    Y_min, Y_max = X_all[:, 1].min() - 0.5, X_all[:, 1].max() + 0.5
    xx, yy = np.meshgrid(
        np.arange(X_min, X_max, h),
        np.arange(Y_min, Y_max, h)
    )
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    plt.figure(figsize=(8, 6))
    plt.contourf(xx, yy, Z, alpha=0.8, cmap='viridis')
    plt.scatter(X_all[:, 0], X_all[:, 1], c=y_all, edgecolors='k', s=80, cmap='viridis')
    plt.xlabel('Feature 1 (scaled)')
    plt.ylabel('Feature 2 (scaled)')
    plt.title("Logistic Regression Decision Boundary (High Accuracy)")
    plt.grid(True)
    plt.show()
else:
    print("Cannot plot decision boundary for data with more than 2 features.")

# Experiment - 09
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.datasets import make_blobs

# Create synthetic dataset
X, y = make_blobs(n_samples=500, n_features=2, centers=3, random_state=23)
df = pd.DataFrame(X, columns=['Feature 1', 'Feature 2'])
df['True cluster'] = y
print("First 5 rows of the synthetic dataset (all columns):")
print(df.head())

# Visualize original data
plt.figure(0)
plt.grid(True)
plt.scatter(X[:, 0], X[:, 1])
plt.title("Generated Data")
plt.show()

# K-means initialization
k = 3
clusters = {}
np.random.seed(23)
for idx in range(k):
    center = 2 * (2 * np.random.random((X.shape[1],)))
    cluster = {
        'center': center,
        'points': []
    }
    clusters[idx] = cluster

# Plot initial centers
plt.scatter(X[:, 0], X[:, 1])
plt.grid(True)
for i in clusters:
    center = clusters[i]['center']
    plt.scatter(center[0], center[1], marker='*', s=200, c='red', label=f"Initial center {i}")
plt.title("Initial Random Cluster Centers")
plt.legend()
plt.show()

# Distance function
def distance(p1, p2):
    return np.sqrt(np.sum((p1 - p2) ** 2))

# Assign points to the nearest cluster
def assign_clusters(X, clusters):
    for idx in range(X.shape[0]):
        curr_x = X[idx]
        dists = [distance(curr_x, clusters[i]['center']) for i in range(k)]
        cluster_idx = np.argmin(dists)
        clusters[cluster_idx]['points'].append(curr_x)
    return clusters

# Update cluster centers
def update_clusters(clusters):
    for i in range(k):
        points = np.array(clusters[i]['points'])
        if points.shape[0] > 0:
            new_center = points.mean(axis=0)
            clusters[i]['center'] = new_center
        clusters[i]['points'] = []
    return clusters

# Predict clusters for each point
def pred_cluster(X, clusters):
    preds = []
    for i in range(X.shape[0]):
        dists = [distance(X[i], clusters[j]['center']) for j in range(k)]
        preds.append(np.argmin(dists))
    return preds

# K-means iteration
n_iters = 10
for i in range(n_iters):
    clusters = assign_clusters(X, clusters)
    clusters = update_clusters(clusters)

# Get final cluster predictions
pred = pred_cluster(X, clusters)

# Plot final clustered data
plt.scatter(X[:, 0], X[:, 1], c=pred, cmap='viridis')
for i in clusters:
    center = clusters[i]['center']
    plt.scatter(center[0], center[1], marker='*', s=200, c='red', label=f"Final center {i}")
plt.title("Final Clustered Data (K-Means)")
plt.grid(True)
plt.legend()
plt.show()

